# NLP - Machine Reading
|Paper|Conference|Remarks
|--|--|--|
|[Long Short-Term Memory-Networks for Machine Reading](https://aclweb.org/anthology/D16-1053)|EMNLP 2017|A novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. 1. A hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. 2. Uses coverage to keep track of what has been summarized, which discourages repetition.|

[Back to index](../README.md)

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTI0MDU3MDc3M119
-->