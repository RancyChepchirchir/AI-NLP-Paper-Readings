# NLP - Text Summarization
|Paper|Conference|Remarks
|--|--|--|
|[Get To The Point: Summarization with Pointer-Generator Networks](https://aclanthology.info/papers/P17-1099/p17-1099)|ACL 2017|A novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. 1. A hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. 2. Uses coverage to keep track of what has been summarized, which discourages repetition.|
|[SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents](http://www.kdd.org/exploration_files/19-2-Article3.pdf)|ACM SIGKDD Explorations Newsletter 2017| 1. Task-oriented and non-task oriented models. 2. How deep learning help the representation. 3. Some appealing research directions.|
|[Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16455/15753)|AAAI 2018|1. Models high-level abstraction of emotion expressions by embedding emotion categories. 2. Captures the change of implicit internal emotion states. 3. Uses explicit emotion expressions with an external emotion vocabulary|
|[Affective Neural Response Generation](https://arxiv.org/abs/1709.03968)|ECIR 2018| 1.  Propose three novel ways to incorporate affective aspects into LSTM encoder-decoder neural conversation models: Affective word embeddings, affect-based objective functions, affectively diverse beam search for decoding. 2. Experiments show that the proposed model produce emotionally rich responses that are more interesting and natural|

[Back to index](../README.md)

<!--stackedit_data:
eyJoaXN0b3J5IjpbMjEyMDIyMjQ4N119
-->