# NLP - Machine Translation
|Paper|Conference|Remarks
|--|--|--|
|[Neural Turing Machines](https://arxiv.org/pdf/1410.5401)|Arxiv 2014|Coupling neural networks to external memory resources, which they can interact with by attentional processes.|
|[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473)|ICLR 2015|Conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly.|
|[Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025)|EMNLP 2015|Examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time.|
|[# Modeling Coverage for Neural Machine Translation](https://arxiv.org/pdf/1601.04811)|ACL 2016|Propose a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust future attention, which lets NMT system to consider more about untranslated source words.|
|[Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/pdf/1609.08144)|Arxiv 2016|Present GNMT, Google's Neural Machine Translation system to improve parallelism, accelerate the final translation speed, improve handling of rare words and encourages generation of an output sentence that is most likely to cover all the words in the source sentence|


[Back to index](../README.md)

<!--stackedit_data:
eyJoaXN0b3J5IjpbOTY0OTkxNzIzLDIxMjQ1MDgwOTksLTExNz
gxMzU2MTIsNzc1OTM0NTgxXX0=
-->