# NLP - Machine Translation
|Paper|Conference|Remarks
|--|--|--|
|[Neural Turing Machines](https://arxiv.org/pdf/1410.5401)|Arxiv 2014|Coupling neural networks to external memory resources, which they can interact with by attentional processes.|
|[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473)|ICLR 2015|Conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly.|
|[Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025)|EMNLP 2015|Examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time.|
|[Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025)|EMNLP 2015|Examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time.|


[Back to index](../README.md)

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTIxMTAzODU2NywyMTI0NTA4MDk5LC0xMT
c4MTM1NjEyLDc3NTkzNDU4MV19
-->