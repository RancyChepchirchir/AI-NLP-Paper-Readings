# NLP - Text Classification
|Paper|Conference|Remarks
|--|--|--|
|[Hierarchical Attention Networks for Document Classification](https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf)|NAACL 2016|1. Propose a hierarchical attention network for document classification. 2. It has a hierarchical structure that mirrors the hierarchical structure of documents. 3. It has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. 4. Unclear about the interpretation of learned attention word and sentence.|
|[Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/pdf/1801.06146)|ACL 2018| 1. Propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. 2. Significantly outperforms the state-of-the-art on six text classification tasks|


[Back to index](../README.md)

<!--stackedit_data:
eyJoaXN0b3J5IjpbMjM3Nzg4Mjc5XX0=
-->