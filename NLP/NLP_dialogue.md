# NLP - Dialogue System
|Paper|Conference|Remarks
|--|--|--|
|[A Neural Conversational Model](https://arxiv.org/pdf/1506.05869)|Arxiv 2015|1. Present a simple approach for conversation modelling which uses the recently proposed sequence to sequence framework. 2. It can be trained end-to-end and thus requires much fewer hand-crafted rules|
|[A Persona-Based Neural Conversation Model](http://www.aclweb.org/anthology/P16-1094)|ACL 2016|1. Present persona-based models for handling the issue of speaker consistency in neural response generation. 2. Yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models, with similar gains in speaker consistency as measured by human judges.|
|[Deep Reinforcement Learning for Dialogue Generation](https://aclweb.org/anthology/D16-1127)|EMNLP 2016|1. Current conversational agents tend to be short-sighted and ignoring long-term interactivity. 2. Combination of traditional NLP model with reinforcement learning to model future rewards in chatbot dialogue to learn a neural conversational model based on long-term success of dialogues. 3. Rewards: informative, coherence and ease-of-answering. 4. Model evaluation metrices: diversity, conversation length and human judges|
|[Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/11957/12160)|AAAI 2016|1. Extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. 2. Investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings|
|[A Simple, Fast Diverse Decoding Algorithm for Neural Generation](https://arxiv.org/pdf/1611.08562)|Arxiv 2016|1. Propose a simple, fast decoding algorithm that fosters diversity in neural generation. 2. Diverse decoding helps across tasks of dialogue response generation, abstractive summarization and machine translation, especially those for which reranking is needed|
|[A Survey on Dialogue Systems: Recent Advances and New Frontiers](http://www.kdd.org/exploration_files/19-2-Article3.pdf)|ACM SIGKDD Explorations Newsletter 2017| 1. Task-oriented and non-task oriented models. 2. How deep learning help the representation. 3. Some appealing research directions.|
|[Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16455/15753)|AAAI 2018|1. Models high-level abstraction of emotion expressions by embedding emotion categories. 2. Captures the change of implicit internal emotion states. 3. Uses explicit emotion expressions with an external emotion vocabulary|
|[Affective Neural Response Generation](https://arxiv.org/abs/1709.03968)|ECIR 2018| 1.  Propose three novel ways to incorporate affective aspects into LSTM encoder-decoder neural conversation models: Affective word embeddings, affect-based objective functions, affectively diverse beam search for decoding. 2. Experiments show that the proposed model produce emotionally rich responses that are more interesting and natural|

[Back to index](../README.md)

<!--stackedit_data:
eyJoaXN0b3J5IjpbMTM0NDU4NzQxMiwtOTU2NzkxMTc0XX0=
-->