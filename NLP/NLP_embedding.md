# NLP - Word, Sentence and Document Embedding
|Paper|Conference|Remarks
|--|--|--|
|[Learning Word Vectors for Sentiment Analysis](https://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf)|HLT 2011|1. Present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term--document information as well as rich sentiment content. 2. Instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents|
|[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781)|Arxiv 2013|A pioneering work that proposes two novel model architectures, namely CBOW and SkipGram, for computing continuous vector representations of words from very large data sets. |
|[Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)|NIPS 2013| Present several extensions that improve both the quality of the vectors and the training speed of SkipGram architecture. Subsampling of frequent words obtain faster training and better performance. Negative sampling uses a few negative samples when computing errors to accelerate optimization and improve performance.|
|[Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification](http://www.aclweb.org/anthology/P14-1146)|ACL 2014|Use a simple MLP with one hidden layer and one softmax layer to predict the sentiment of a sliding ngram of each sentence. The loss function is the weighed sum of cross-entropy loss and hinge loss.|
|[GloVe: Global Vectors for Word Representation](https://www.aclweb.org/anthology/D14-1162)|EMNLP 2014|1. A new global log-bilinear regression model that combines the advantages of two major model families in the literature: global matrix factorization and local context window methods|
|[Specializing Word Embeddings for Similarity or Relatedness](http://aclweb.org/anthology/D15-1242)|EMNLP 2015|1. Demonstrate the advantage of specializing semantic word embeddings for either similarity or relatedness. 2.  Find that retrofitting and joint-learning approaches yield specialized semantic spaces and perform better than unspecialized spaces|
|[Retrofitting Word Vectors to Semantic Lexicons](https://arxiv.org/pdf/1411.4166)|NAACL 2015|By utilizing external semantic lexicons such as WordNet, the model aims to compute new word embeddings that are both close to their counterparts in the original word embeddings and to adjacent words in external semantic lexicons.|
|[A Simple But Tough-to-Beat Baseline for Sentence Embeddings](https://openreview.net/pdf?id=SyK00v5xx)|ICLR 2016| 1. Proposed a completely unsupervised sentence embedding method. 2. Use word embeddings computed using one of the popular methods on unlabeled corpus like Wikipedia, represent the sentence by a weighted average of the word vectors, and then modify them using PCA/SVD. 3. This weighting improves performance by about 10% to 30% in textual similarity tasks. 4. This paper also gives a theoretical explanation of the success using a latent variable generative model for sentences|
|[Sentiment Embeddings with Applications to Sentiment Analysis](http://millenniumsoftsol.com/courses/IEEETitles/Java/Sentiment-Embeddings.pdf)|IEEE TKDE 2016| 1. Encode sentiment information of texts (e.g., sentences and words) together with contexts of words in sentiment embeddings. 2. Develop a number of neural networks with tailoring loss functions, and collect massive texts automatically with sentiment signals like emoticons as the training data.|
|[Towards Building Affect Sensitive Word Distributions](https://openreview.net/pdf?id=By5SY2gA-)|N.A.|1. To incorporate affect lexica, which capture fine-grained information about a word's psycholinguistic and emotional orientation, into the training process of Word2Vec and GloVe using a joint learning approach. 2. The proposed method outperforms previous work on standard tasks such as word similarity detection, outlier detection and sentiment detection.|
|[Enriching Word Vectors with Subword Information](http://aclweb.org/anthology/Q17-1010)|ACL 2017|Present a variant of SkipGram model to take subword information into account when training word embeddings. Specifically, each n-gram is associated with one vector and each word is the sum of its comprising n-gram vectors. The similarity score between two words is calculated as the sum of scores between a word and the n-grams of the other word.|
|[A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings](http://aclweb.org/anthology/D17-1312)|EMNLP 2017|Present a simple yet effective method for learning word embeddings based on text from different domains.|
|[Can Word Embeddings Help Find Latent Emotions in Text](https://aaai.org/ocs/index.php/FLAIRS/FLAIRS17/paper/view/15516/14967)|AAAI 2017|The results conclude that existing word embeddings are unable to deliver emotion information. For example, the arithmetic joy + fear = guilt does not hold. Also, emotionally similar words are far apart in the word embedding space.|
|[A Structured Self-attentive Sentence Embedding](https://arxiv.org/pdf/1703.03130)|ICLR 2017|1. Proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. 2. Use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence.|
|[Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks](https://arxiv.org/pdf/1608.04207)|ICLR 2017|1. Propose a framework that facilitates better understanding of the encoded representations of sentences. 2. Define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when using the representation as input.|
|[FRAGE: Frequency-Agnostic Word Representation](https://arxiv.org/abs/1809.06858)|NIPS 2018|1. Identifies the problem that the embeddings for popular and rare words lie in different subregions in the vector space. 2. Proposes an adversarial training method that adds an additional loss that tries to train word embeddings to fool the discriminator - a popular/rare word classifier. |
|[Domain Adapted Word Embeddings for Improved Sentiment Classification](http://www.aclweb.org/anthology/W18-3407)|ACL 2018|Use Canonical Correlation Analysis (CCA) to combine generic word embeddings with domain specific word embeddings. The domain specific embeddings are obtained via LSA on domain specific data.|
|[Learning Domain-Sensitive and Sentiment-Aware Word Embeddings](http://aclweb.org/anthology/P18-1232)|ACL 2018|Create embeddings for generic domain and each specific domain. A latent variable is introduced for each word to indicate its probability of being in common domain. This paper extends skip-gram model to predict the polarity of each word. All embeddings are learned via EM algorithm. |
|[Learning Emotion-enriched Word Representations](http://www.aclweb.org/anthology/C18-1081)|COLING 2018|The emotion-rich embeddings are learned by training LSTM with cross-entropy loss to predict the emotion label of each document. Each word in the document is fed sequentially into the LSTM model. The initial embeddings are either randomly initialized or loaded from pre-trained embeddings. |
|[Uncovering divergent linguistic information in word embeddings with lessons for intrinsic and extrinsic evaluation](https://aclanthology.info/papers/K18-1028/k18-1028)|CoNLL 2018|1. Argues that word embeddings have two aspects: semantics/syntax axis and similarity/relatedness axis. Each aspect has somewhat incompatible features. They conduct experiments showing that existing popular word embeddings such as Word2Vec, GloVe and FastText all have captured these information but have different surface forms. 2. Proposes a simple linear transformation as a post-processing technique that can adjust existing word embeddings towards specific axes (e.g., more semantic info vs more syntax info, or more similarity info vs more relateness info). |
|[Learning Sentiment-Specific Word Embedding via Global Sentiment Representation](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPDFInterstitial/16334/16010)|AAAI 2018|Extensions to CBoW model, which predicts center word based on context words. The proposed model additionally considers document vector when predicting the center word, where the document vector is weighed sum of word vectors. The overall loss is weighted sum of cross-entropy losses for center word prediction and sentiment polarity prediction.|
|[A Survey of Word Embeddings Evaluation Methods](https://arxiv.org/abs/1801.09536)|Arxiv 2018|1. Explores issues in existing evaluation methods for word embeddings: obsecureness of the notion of semantics, lack of proper training data, absence of correlation between intrinsic and extrinsic methods, etc. 2. Reviews 16 intrinsic methods and 12 extrinsic methods for word embedding evaluation. 3. Summarizes common datasets used for each evaluation method.|

[Back to index](../README.md)

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTc2NTcyMjc2MCwtMTM3MjY5NTY2MywtMT
U3MjAxOTQ3NSwtMTY5Mzg0NjUzMywtMTQwMDU5MzQ2NiwxMjg1
ODIwOTI1LC0xNDU1OTk5NTQ5LDE1NzY4NjY1MzQsLTE3Nzc2MT
kxMTRdfQ==
-->