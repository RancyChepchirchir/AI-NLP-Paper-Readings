# NLP - Word, Sentence and Document Embedding
|Paper|Conference|Remarks
|--|--|--|
|[GloVe: Global Vectors for Word Representation](https://www.aclweb.org/anthology/D14-1162)|EMNLP 2014|1.  A new global log-bilinear regression model that combines the advantages of two major model families in the literature: global matrix factorization and local context window methods|
|[A Simple But Tough-to-Beat Baseline for Sentence Embeddings](https://openreview.net/pdf?id=SyK00v5xx)|ICLR 2016| 1. Proposed a completely unsupervised sentence embedding method. 2. Use word embeddings computed using one of the popular methods on unlabeled corpus like Wikipedia, represent the sentence by a weighted average of the word vectors, and then modify them using PCA/SVD. 3. This weighting improves performance by about 10% to 30% in textual similarity tasks. 4. This paper also gives a theoretical explanation of the success using a latent variable generative model for sentences|
|[Towards Building Affect Sensitive Word Distributions]|||

[Back to index](../README.md)

<!--stackedit_data:
eyJoaXN0b3J5IjpbNzA3ODUxNDE1XX0=
-->