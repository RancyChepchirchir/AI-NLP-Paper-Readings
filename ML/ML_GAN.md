# ML - Generative Adversatial Network (GAN)
|Paper|Conference|Remarks
|--|--|--|
|[Generative Adversarial Nets](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)|NIPS 2014|A new framework for estimating generative models via an adversarial process, in which they simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G.|
|[Improved Techniques for Training GANs](https://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf)|NIPS 2016| Presents a variety of new architectural features and training procedures that weapply to the generative adversarial networks (GANs) framework. |
|[Tutorial - Generative Adversatial Network](https://arxiv.org/abs/1701.00160)|NIPS Workshop 2016| 1. Why generative modeling is a topic worth studying. 2. How generative models work, and how GANs compare to other generative models. 3. The details of how GANs work. 4. Research frontiers in GANs. 5. State-of-the-art image models that combine GANs with other methods|
|[Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/pdf/1511.06434)|ICLR 2016|Introduces a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning|
|[Generative Adversarial Networks: An Overview](https://arxiv.org/pdf/1710.07035)|IEEE SPM 2017|1. Provides an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. 2. Points out remaining challenges.|
|[Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/pdf/1611.07004)|CVPR 2017| 1. Cast conditional adversarial networks as a general-purpose solution to image-to-image translation problems. 2. This approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks.|
|[Wasserstein GAN](https://arxiv.org/pdf/1701.07875)|ICML 2017|1. It uses Wasserstein distance as distances between two distributions to ensure continuity an. 2. WGAN can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches.|
|[Learning Options in Multi-objective Reinforcement Learning](https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14727/14148)|AAAI 2017| 1. Propose a method to learn options in Multi-objective RL domains in order to accelerate learning and reuse knowledge across tasks. 2. The main idea is to learn options for each objective separately (PolicyBlocks can be used) and apply the learned options to the multi-objective problem|
|[Knowledge Transfer for Deep Reinforcement Learning with Hierarchical Experience Replay](http://www.aaai.org/Conferences/AAAI/2017/PreliminaryPapers/11-YinH-14478.pdf)|AAAI 2017| 1. A new policy distillation architecture for knowledge sharing in deep reinforcement learning in multi-task domains. 2. A hierarchical prioritized experience replay in memory replay of DQN|
|[Transfer Reinforcement Learning with Shared Dynamics](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14315/14386)|AAAI 2017| 1. Focus on a particular Transfer RL problem: dynamics do not change from one task to another, and only the reward function does. 2. First idea: transition samples obtained from one task can be reused to learn on any other task: an immediate reward estimator is learnt in a supervised fashion. 3. Second idea: adopt optimism in the face of uncertainty to encourage exploration|
|[FeUdal Network for Hierarchical Reinforcement Learning](https://arxiv.org/abs/1703.01161)|ICML 2017| 1. Propose a novel architecture for hierarchical reinforcement learning. 2. Employs a Manager module and a Worker module. 3. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. 4. The Worker generates primitive actions at every tick of the environment. 5. This FuN facilitates very long timescale credit assignment and encourages the emergence of sub-policies associated with different goals set by the Manager|
|[Stochastic Neural Networks for Hierarchical Reinforcement Learning](https://arxiv.org/abs/1704.03012)|ICLR 2017| A general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks to tackle sparse rewards and long horizons.|

[Back to index](../README.md)
<!--stackedit_data:
eyJoaXN0b3J5IjpbNzA5NTU2MzM2XX0=
-->