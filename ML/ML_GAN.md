# ML - Generative Adversatial Network (GAN)
|Paper|Conference|Remarks
|--|--|--|
|[Generative Adversarial Nets](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)|NIPS 2014|A new framework for estimating generative models via an adversarial process, in which they simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G.|
|[Improved Techniques for Training GANs](https://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf)|NIPS 2016| Presents a variety of new architectural features and training procedures that weapply to the generative adversarial networks (GANs) framework. |
|[Tutorial - Generative Adversatial Network](https://arxiv.org/abs/1701.00160)|NIPS Workshop 2016| 1.  Asynchronous gradient descent for optimization of deep neural network controllers. 2. The asynchronous method can be applied to both value-based and policy-based models, off-policy as well as on-policy, and in discrete as well as continuous domains. 3. Overall Asynchronous Advantage Actor-Critic (A3C) model has best performance|
|[Accelerating Multiagent Reinforcement Learning through Transfer Learning](https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14217/14005)|AAAI 2017| 1. RL algorithms suffer from scalability issues, especially in a MAS. 2. Objective: accelerate learning in multiagent sequential decision making tasks by reusing previous knowledge, both from past solutions and advising between agents|
|[An Advising Framework for Multiagent Reinforcement Learning Systems](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14413)|AAAI 2017|1. Classical RL approaches for sequential decision making in MAS require a long time to learn. 2. Objective: propose an advising framework where multiple agents advise each other while learning in a shared environment, starting with no previous knowledge, and the advisor is not expected to necessarily act optimally.|
|[An Efficient Approach to Model-Based Hierarchical Reinforcement Learning](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14771/14160)|AAAI 2017| 1. Propose a model-based approach to hierarchical reinforcement learning that exploits shared knowledge and selective execution at different levels of abstraction. 2. The proposed framework adopts a new transmission dynamics learning algorithm that identifies the common action-feature combinations of the subtasks, and evaluates the subtask execution choices through simulation.|
|[Improving Deep Reinforcement Learning with Knowledge Transfer](http://www.cowhi.org/docs/2017-02-04_AAAI_2017_DC_Slides.pdf)|AAAI 2017| 1. While DRL has achieved good results in single task learning, the multi-task case is still underrepresented in the available literature. 2. This research proposal aims at extending DRL to the multi-task case by leveraging the power of Transfer Learning (TL) to improve the training time and results. 3. The focus is to define a novel framework for scalable DRL agents that detects similarities between tasks and balances various TL techniques, such as parameter sharing, policy transfer and skill transfer|
|[Learning Options in Multi-objective Reinforcement Learning](https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14727/14148)|AAAI 2017| 1. Propose a method to learn options in Multi-objective RL domains in order to accelerate learning and reuse knowledge across tasks. 2. The main idea is to learn options for each objective separately (PolicyBlocks can be used) and apply the learned options to the multi-objective problem|
|[Knowledge Transfer for Deep Reinforcement Learning with Hierarchical Experience Replay](http://www.aaai.org/Conferences/AAAI/2017/PreliminaryPapers/11-YinH-14478.pdf)|AAAI 2017| 1. A new policy distillation architecture for knowledge sharing in deep reinforcement learning in multi-task domains. 2. A hierarchical prioritized experience replay in memory replay of DQN|
|[Transfer Reinforcement Learning with Shared Dynamics](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14315/14386)|AAAI 2017| 1. Focus on a particular Transfer RL problem: dynamics do not change from one task to another, and only the reward function does. 2. First idea: transition samples obtained from one task can be reused to learn on any other task: an immediate reward estimator is learnt in a supervised fashion. 3. Second idea: adopt optimism in the face of uncertainty to encourage exploration|
|[FeUdal Network for Hierarchical Reinforcement Learning](https://arxiv.org/abs/1703.01161)|ICML 2017| 1. Propose a novel architecture for hierarchical reinforcement learning. 2. Employs a Manager module and a Worker module. 3. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. 4. The Worker generates primitive actions at every tick of the environment. 5. This FuN facilitates very long timescale credit assignment and encourages the emergence of sub-policies associated with different goals set by the Manager|
|[Stochastic Neural Networks for Hierarchical Reinforcement Learning](https://arxiv.org/abs/1704.03012)|ICLR 2017| A general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks to tackle sparse rewards and long horizons.|

[Back to index](../README.md)
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTc0NTY0NzU2MV19
-->