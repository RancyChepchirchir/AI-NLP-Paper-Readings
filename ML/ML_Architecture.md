# ML - Architecture
|Paper|Conference|Remarks
|--|--|--|
|[Neural Turing Machines](https://arxiv.org/pdf/1410.5401)|Arxiv 2014|Coupling neural networks to external memory resources, which they can interact with by attentional processes.|
|[Hybrid Computing Using a Neural Network with Dynamic External Memory](https://arxiv.org/pdf/1502.03167)|Arxiv 2015| 1. The distribution of each layer's inputs changes during training, which slows down the training by requiring lower learning rates and careful parameter initialization. 2. BN performs normalization for each training mini-batch and allows us to use much higher learning rate and be less careful about initialization|
|[An Overview of Gradient Descent Optimization Algorithms](https://pdfs.semanticscholar.org/e2dc/8810671f76927d862e63faa29c401bdec5da.pdf)|Arxiv 2016| 1. Stochastic vs Batch vs Minibatch Gradient Descent. 2. It introduces momentum, Nesterov accelerated momentum, Adagrad, Adadelta, RMSProps and Adam optimizations.|

[Back to index](../README.md)
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTczNTY1MTQwM119
-->