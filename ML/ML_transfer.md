# ML - Transfer Learning
|Paper|Conference|Remarks
|--|--|--|
|[A Kernel Two-Sample Test](http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf)|JMLR 2012| 1. Propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. 2. The proposed test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the _maximum mean discrepancy_ (MMD).|
|[Generative Moment Matching Networks](https://arxiv.org/abs/1502.02761)|ICML 2015| 1. Formulate a method that generates an independent sample via a single feedforward pass through a multilayer perceptron, as in the GAN. 2. Utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. 3. Further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples.|
|[A Survey on Transfer Learning](https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf)|TKDE 2009| 1. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression and clustering problems. 2. Discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as co-variate shift.|
|[Learning Transferable Features with Deep Adaptation Networks](https://arxiv.org/abs/1502.02791)|ICML 2015| 1. Propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario, where hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. 2. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding.|
|[Domain-Adversarial Training of Neural Networks](http://jmlr.org/papers/volume17/15-239/15-239.pdf)|JMLR 2016| 1. Introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. 2. The approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. 3. Show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages.|
|[Adversarial Discriminative Domain Adaptation](https://arxiv.org/abs/1702.05464)|CVPR 2017| 1. Propose a general framework which combines discriminative modeling, untied weight sharing, and a GAN loss, called Adversarial Discriminative Domain Adaptation (ADDA). 2. Show that ADDA is more effective yet considerably simpler than competing domain-adversarial methods, and demonstrate the promise of our approach by exceeding state-of-the-art unsupervised adaptation results on standard cross-domain digit classification tasks and a new more difficult cross-modality object classification task.|
|[A Kernel Two-Sample Test](http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf)|JMLR 2012| 1. Propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. 2. The proposed test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the _maximum mean discrepancy_ (MMD).|
|[A Kernel Two-Sample Test](http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf)|JMLR 2012| 1. Propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. 2. The proposed test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the _maximum mean discrepancy_ (MMD).|
<!--stackedit_data:
eyJoaXN0b3J5IjpbNzMyOTEyMDEwLDczMDk5ODExNl19
-->