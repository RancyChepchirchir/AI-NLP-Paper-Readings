# ML - Reinforcement Learning
|Paper|Conference|Remarks
|--|--|--|
|[Human-level control through deep reinforcement learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf)|Nature 2015|1. RL challenges: derivation of efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. 2.  This paper: use deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. 3. RL instability causes: the correlations present in the sequence of observations; the fact that small updates to Q may significantly change the policy and therefore change the data distribution; the correlations between Q and target values. 4. RL instability solutions: experience replay that randomize over data and therefore removing correlations in the observation sequence and smoothing over changes in the data distribution; only periodically adjust Q towards target values to reduce correlations with the target|
|[Mastering the game of Go with deep neural networks and tree search](https://deepmind.com/documents/119/agz_unformatted_nature.pdf)|Nature 2016| 1. Challenges of Go: enormous search space, difficulty of evaluating board positions and moves. 2. Train SL policy network directly from human expert moves. 3. Train a fast policy that can rapidly sample actions during rollouts. 4. Train a RL policy network that improves SL policy network by optimizing final outcome. 5. Train a value network that predicts the winner of games played by the RL policy network against itself|
|[Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/pdf/1602.01783)|JMLR 2016| 1.  Asynchronous gradient descent for optimization of deep neural network controllers. 2. The asynchronous method can be applied to both value-based and policy-based models, off-policy as well as on-policy, and in discrete as well as continuous domains. 3. Overall Asynchronous Advantage Actor-Critic (A3C) model has best performance|
|[Accelerating Multiagent Reinforcement Learning through Transfer Learning](https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14217/14005)|AAAI 2017| 1. RL algorithms suffer from scalability issues, especially in a MAS. 2. Objective: accelerate learning in multiagent sequential decision making tasks by reusing previous knowledge, both from past solutions and advising between agents|
|[An Advising Framework for Multiagent Reinforcement Learning Systems](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14413)|AAAI 2017|1. Classical RL approaches for sequential decision making in MAS require a long time to learn. 2. Objective: propose an advising framework where multiple agents advise each other while learning in a shared environment, starting with no previous knowledge, and the advisor is not expected to necessarily act optimally.|
|[An Efficient Approach to Model-Based Hierarchical Reinforcement Learning](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14771/14160)|AAAI 2017| 1. Propose a model-based approach to hierarchical reinforcement learning that exploits shared knowledge and selective execution at different levels of abstraction. 2. The proposed framework adopts a new transmission dynamics learning algorithm that identifies the common action-feature combinations of the subtasks, and evaluates the subtask execution choices through simulation.|
|[Improving Deep Reinforcement Learning with Knowledge Transfer](http://www.cowhi.org/docs/2017-02-04_AAAI_2017_DC_Slides.pdf)|AAAI 2017| 1. While DRL has achieved good results in single task learning, the multi-task case is still underrepresented in the available literature. 2. This research proposal aims at extending DRL to the multi-task case by leveraging the power of Transfer Learning (TL) to improve the training time and results. 3. The focus is to define a novel framework for scalable DRL agents that detects similarities between tasks and balances various TL techniques, such as parameter sharing, policy transfer and skill transfer|
|[An Efficient Approach to Model-Based Hierarchical Reinforcement Learning](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14771/14160)|AAAI 2017| 1. Propose a model-based approach to hierarchical reinforcement learning that exploits shared knowledge and selective execution at different levels of abstraction. 2. The proposed framework adopts a new transmission dynamics learning algorithm that identifies the common action-feature combinations of the subtasks, and evaluates the subtask execution choices through simulation.||[An Efficient Approach to Model-Based Hierarchical Reinforcement Learning](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14771/14160)|AAAI 2017| 1. Propose a model-based approach to hierarchical reinforcement learning that exploits shared knowledge and selective execution at different levels of abstraction. 2. The proposed framework adopts a new transmission dynamics learning algorithm that identifies the common action-feature combinations of the subtasks, and evaluates the subtask execution choices through simulation.|
[Back to index](../README.md)
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE5NDQ1ODU0OTcsMTMxNTYwNjA1Ml19
-->